{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for cleaning a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering complete. The filtered CSV file has been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the function based on the provided keypoint indices\n",
    "def keypoint_to_face_part(index):\n",
    "    if 1 <= index <= 33:\n",
    "        return 0  # Do not load in df\n",
    "    elif 34 <= index <= 42:\n",
    "        return \"Right_brow\"\n",
    "    elif 43 <= index <= 51:\n",
    "        return \"Left_brow\"\n",
    "    elif 52 <= index <= 66:\n",
    "        if 56 <= index <= 66:\n",
    "            return 0  # Do not load in df\n",
    "        return \"Nose\"\n",
    "    elif 67 <= index <= 75:\n",
    "        return \"Right_Eye\"\n",
    "    elif 76 <= index <= 84:\n",
    "        return \"Left_Eye\"\n",
    "    elif 85 <= index <= 104:\n",
    "        return \"Mouth\"\n",
    "    elif index == 105 or index == 106:\n",
    "        return 0  # Do not load in df\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = \"/workspaces/wiggle-face/data/panda_data/keypoints_clean_PANDA2_dataset.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Filter out the rows with the specified keypoint indices\n",
    "df['face_part'] = df['keypoint_index'].apply(keypoint_to_face_part)\n",
    "filtered_df = df[df['face_part'] != 0]\n",
    "\n",
    "# Drop the 'face_part' column as it's no longer needed\n",
    "filtered_df = filtered_df.drop(columns=['face_part'])\n",
    "\n",
    "# Save the filtered DataFrame back to a CSV file if needed\n",
    "filtered_csv_file_path = \"/workspaces/wiggle-face/data/panda_data/panda_new/keypoints_clean_PANDA2_2_dataset.csv\"\n",
    "filtered_df.to_csv(filtered_csv_file_path, index=False)\n",
    "\n",
    "print(\"Filtering complete. The filtered CSV file has been saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Loading and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from scipy.ndimage import median_filter\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsDataset:\n",
    "    def __init__(self, json_dir, batch_size=200000):\n",
    "        self.json_dir = json_dir\n",
    "        self.json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def keypoint_to_face_part(self, index):\n",
    "        if 1 <= index <= 33:\n",
    "            return 0  # Do not load in df\n",
    "        elif 34 <= index <= 42:\n",
    "            return \"Right_brow\"\n",
    "        elif 43 <= index <= 51:\n",
    "            return \"Left_brow\"\n",
    "        elif 52 <= index <= 66:\n",
    "            if 56 <= index <= 66:\n",
    "                return 0  # Do not load in df\n",
    "            return \"Nose\"\n",
    "        elif 67 <= index <= 75:\n",
    "            return \"Right_Eye\"\n",
    "        elif 76 <= index <= 84:\n",
    "            return \"Left_Eye\"\n",
    "        elif 85 <= index <= 104:\n",
    "            return \"Mouth\"\n",
    "        elif index == 105 or index == 106:\n",
    "            return 0  # Do not load in df\n",
    "\n",
    "    def apply_filters(self, df):\n",
    "        # Apply mean filter across frames for each keypoint index\n",
    "        df['mean_keypoint_x'] = df.groupby('keypoint_index')['keypoint_x'].rolling(window=3, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        df['mean_keypoint_y'] = df.groupby('keypoint_index')['keypoint_y'].rolling(window=3, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        \n",
    "        # Apply median filter on the mean-filtered data across frames for each keypoint index\n",
    "        df['median_keypoint_x'] = df.groupby('keypoint_index')['mean_keypoint_x'].transform(lambda x: median_filter(x, size=3))\n",
    "        df['median_keypoint_y'] = df.groupby('keypoint_index')['mean_keypoint_y'].transform(lambda x: median_filter(x, size=3))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def load_data(self):\n",
    "        data = []\n",
    "        for json_file in self.json_files:\n",
    "            video_id = os.path.splitext(json_file)[0]\n",
    "            parts = video_id.split('_')\n",
    "            \n",
    "            infant_id = parts[0]\n",
    "            if parts[6] == 'week':\n",
    "                age = int(parts[4])  \n",
    "            else :\n",
    "                age = int(parts[4]) * 4\n",
    "\n",
    "            print(f\"Processing infant ID: {infant_id}, age in weeks: {age}\")\n",
    "\n",
    "            with open(os.path.join(self.json_dir, json_file), 'r') as f:\n",
    "                frames = json.load(f)\n",
    "                frame_data_list = []\n",
    "\n",
    "                for frame_index, frame_data in enumerate(frames):\n",
    "                    frame_id = frame_data[\"frame_id\"]\n",
    "                    for instance_index, instance in enumerate(frame_data[\"instances\"]):\n",
    "                        keypoints = instance[\"keypoints\"]\n",
    "                        keypoint_scores = instance[\"keypoint_scores\"]\n",
    "\n",
    "                        for idx, (kp, score) in enumerate(zip(keypoints, keypoint_scores)):\n",
    "                            face_part = self.keypoint_to_face_part(idx + 1)\n",
    "                            if face_part != 0:\n",
    "                                frame_data_list.append({\n",
    "                                    \"infant_id\": int(infant_id),\n",
    "                                    \"age_weeks\": age,\n",
    "                                    \"frame_id\": frame_id,\n",
    "                                    \"keypoint\": tuple(kp),\n",
    "                                    \"keypoint_score\": score,\n",
    "                                    \"face_part\": face_part,\n",
    "                                    \"keypoint_index\": idx + 1,\n",
    "                                    \"keypoint_x\": kp[0],\n",
    "                                    \"keypoint_y\": kp[1]\n",
    "                                })\n",
    "\n",
    "                # Convert to DataFrame for processing\n",
    "                df = pd.DataFrame(frame_data_list)\n",
    "\n",
    "                # Apply filters to smooth the keypoints\n",
    "                df = self.apply_filters(df)\n",
    "\n",
    "                # Remove frames with average keypoint confidence under 0.8\n",
    "                for frame_id, frame_group in df.groupby('frame_id'):\n",
    "                    if frame_group['keypoint_score'].all() >= 0.75:\n",
    "                        for _, row in frame_group.iterrows():\n",
    "                            data.append({\n",
    "                                \"infant_id\": row[\"infant_id\"],\n",
    "                                \"age_weeks\": row[\"age_weeks\"],\n",
    "                                \"frame_id\": row[\"frame_id\"],\n",
    "                                \"keypoint\": (row['median_keypoint_x'], row['median_keypoint_y']),\n",
    "                                \"keypoint_score\": row[\"keypoint_score\"],\n",
    "                                \"face_part\": row[\"face_part\"],\n",
    "                                \"keypoint_index\": row[\"keypoint_index\"]\n",
    "                            })\n",
    "\n",
    "                            if len(data) >= self.batch_size:\n",
    "                                yield data\n",
    "                                data = []\n",
    "                                gc.collect()\n",
    "\n",
    "        if data:\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = r'/workspaces/wiggle-face/data-ioana/PANDA2/face/annotations'\n",
    "dataset = KeypointsDataset(json_dir)\n",
    "df_list = []\n",
    "\n",
    "try:\n",
    "    for batch_data in dataset.load_data():\n",
    "        batch_df = pd.DataFrame(batch_data)\n",
    "        df_list.append(batch_df)\n",
    "        gc.collect()  # Collect garbage to free memory\n",
    "\n",
    "    # Concatenate all collected DataFrames at once\n",
    "    if df_list:\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"Final DataFrame created with {len(df)} records\")\n",
    "    else:\n",
    "        print(\"No data processed. DataFrame is empty.\")\n",
    "        df = pd.DataFrame()  # Return an empty DataFrame if no data was processed\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "json_dir = r'/workspaces/wiggle-face/data-ioana/PANDA2/face/annotations'\n",
    "dataset = KeypointsDataset(json_dir)\n",
    "df_list = []\n",
    "\n",
    "for batch_data in dataset.load_data():\n",
    "    batch_df = pd.DataFrame(batch_data)\n",
    "    df_list.append(batch_df)\n",
    "\n",
    "# Concatenate all collected DataFrames at once\n",
    "df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['infant_id','age_weeks','frame_id']).reset_index(drop=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify continuous segments that are at least 10 frames long (roughly 1/3 second) \n",
    "df = df.sort_values(by=['infant_id', 'age_weeks', 'keypoint_index', 'frame_id'])\n",
    "# identify continuous segments for each 'infant_id' and 'keypoint_index'\n",
    "\n",
    "df['frame_diff'] = df.groupby(['infant_id','age_weeks', 'keypoint_index'])['frame_id'].diff().fillna(1)\n",
    "df['block'] = (df['frame_diff'] != 1).cumsum()\n",
    "blocks = df.groupby(['infant_id','age_weeks','keypoint_index', 'block']).filter(lambda x: len(x) >= 10)\n",
    "blocks = blocks.drop(columns=['block', 'frame_diff'])\n",
    "blocks = blocks.rename(columns={\"processed_keypoint\":\"keypoint\"})\n",
    "blocks = blocks[['infant_id', 'age_weeks', 'frame_id', 'keypoint_index', 'keypoint', 'keypoint_score', 'face_part']]\n",
    "blocks = blocks.sort_values(by=['infant_id','age_weeks','frame_id']).reset_index(drop = True)\n",
    "blocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv files to be processed by \"Data_Inspection_Emotion_Analysis\"\n",
    "blocks.to_csv('keypoints_clean_PANDA2_2_dataset.csv', index=False)\n",
    "print(\"Dataset saved successfully as keypoints_clean_PANDA2_2_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
